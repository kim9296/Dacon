{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim9296/Dacon/blob/main/cyp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd1FZe-U9O3E"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "my_path = '/content/notebooks'\n",
        "# Colab Notebooks 안에 my_env 폴더에 패키지 저장\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/my_env', my_path)\n",
        "sys.path.insert(0, my_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxg952id-06-"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/dacon_cyp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPrMtcIC6b1W"
      },
      "outputs": [],
      "source": [
        "# !pip install --target=$my_path utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAn2JrAk_iUo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from rdkit import DataStructs\n",
        "from rdkit.Chem import PandasTools, AllChem\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK9xPMr3Ip51"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZFtiH8fFGD0"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "seed_everything(seed) # Seed 고정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jPmrACf-93P"
      },
      "outputs": [],
      "source": [
        "train_csv = pd.read_csv('data/train.csv')\n",
        "test_csv = pd.read_csv('data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_JS2qN0_Az9"
      },
      "outputs": [],
      "source": [
        "PandasTools.AddMoleculeColumnToFrame(train_csv,'SMILES','Molecule')\n",
        "PandasTools.AddMoleculeColumnToFrame(test_csv,'SMILES','Molecule')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ka4Q9f91S6p"
      },
      "outputs": [],
      "source": [
        " # @title Phase1 / 2 rules\n",
        "\n",
        "glory_rules = pd.read_csv('data/gloryx_reactionrules.csv')\n",
        "rxn_positions_mols = [Chem.MolFromSmarts(x.split('>>')[0]) for x in glory_rules.SMIRKS]\n",
        "print(len(rxn_positions_mols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnAcX_gY7Dvq"
      },
      "outputs": [],
      "source": [
        "vocab_size = 0\n",
        "global vocab_size\n",
        "\n",
        "def mol2vec(mol):\n",
        "  global vocab_size\n",
        "  fp1 = [len(mol.GetSubstructMatches(x)) for x in rxn_positions_mols]\n",
        "  if max(fp1) > vocab_size:\n",
        "    vocab_size = max(fp1)\n",
        "  fp2 = AllChem.GetHashedMorganFingerprint(mol, 4, nBits=2048)\n",
        "  ar = np.zeros((1,), dtype=np.int8)\n",
        "  DataStructs.ConvertToNumpyArray(fp2, ar)\n",
        "  # return np.concatenate((ar, np.array(fp1)))\n",
        "  return np.array(fp1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQGqNQfe7UI4"
      },
      "outputs": [],
      "source": [
        "train_csv[\"FPs\"] = train_csv.Molecule.apply(mol2vec)\n",
        "test_csv[\"FPs\"] = test_csv.Molecule.apply(mol2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9S0CpS7P3SZ"
      },
      "outputs": [],
      "source": [
        "# 사용할 column만 추출\n",
        "train_data = train_csv[['FPs','MLM', 'HLM']]\n",
        "test_data = test_csv[['FPs']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRmm-IdcDOrc"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThMrXK9CFYlN"
      },
      "outputs": [],
      "source": [
        "# def mol2fp(mol):\n",
        "#     fp = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n",
        "#     ar = np.zeros((1,), dtype=np.int8)\n",
        "#     DataStructs.ConvertToNumpyArray(fp, ar)\n",
        "#     return ar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JivjYYsKFasy"
      },
      "outputs": [],
      "source": [
        "# # FPs column 추가\n",
        "# train[\"FPs\"] = train.Molecule.apply(mol2fp)\n",
        "# test[\"FPs\"] = test.Molecule.apply(mol2fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJmdK7hWFe5a"
      },
      "outputs": [],
      "source": [
        "# # 사용할 column만 추출\n",
        "# train = train_csv[['FPs','MLM', 'HLM']]\n",
        "# test = test_csv[['FPs']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIJQq5XXqfo6"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparameter\n",
        "\n",
        "CFG = {'BATCH_SIZE': 256,\n",
        "       'EPOCHS': 1000,\n",
        "       'HIDDEN_SIZE': 1024,\n",
        "       'OUTPUT_SIZE': 1,\n",
        "       'DROPOUT_RATE': 0.8,\n",
        "       'LEARNING_RATE': 0.001,\n",
        "       'SIGMOID' : True,\n",
        "       'BCE' : False}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT4gWPvXiYj0"
      },
      "outputs": [],
      "source": [
        "# @title GraphGCN DataEmbedding\n",
        "\n",
        "LIST_SYMBOLS = ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
        "            'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
        "            'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
        "            'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']\n",
        "\n",
        "\n",
        "def atom_feature(atom):\n",
        "    return np.array(char_to_ix(atom.GetSymbol(), LIST_SYMBOLS) +\n",
        "                    char_to_ix(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
        "                    char_to_ix(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
        "                    char_to_ix(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
        "                    char_to_ix(int(atom.GetIsAromatic()), [0, 1]))    # (40, 6, 5, 6, 2)\n",
        "\n",
        "\n",
        "def char_to_ix(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        return [0] # Unknown Atom Token\n",
        "    return [allowable_set.index(x)+1]\n",
        "\n",
        "\n",
        "def mol2graph(smi, MAX_LEN):\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "\n",
        "    X = np.zeros((MAX_LEN, 5), dtype=np.uint8)\n",
        "    A = np.zeros((MAX_LEN, MAX_LEN), dtype=np.uint8)\n",
        "\n",
        "    temp_A = Chem.rdmolops.GetAdjacencyMatrix(mol).astype(np.uint8, copy=False)[:MAX_LEN, :MAX_LEN]\n",
        "    num_atom = temp_A.shape[0]\n",
        "    A[:num_atom, :num_atom] = temp_A + np.eye(temp_A.shape[0], dtype=np.uint8)\n",
        "\n",
        "    for i, atom in enumerate(mol.GetAtoms()):\n",
        "        feature = atom_feature(atom)\n",
        "        X[i, :] = feature\n",
        "        if i + 1 >= num_atom: break\n",
        "\n",
        "    return X, A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI1TQzigFimV"
      },
      "outputs": [],
      "source": [
        "# @title Custom Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, target, transform, is_test=False, is_sigmoid = False, is_bce = False):\n",
        "        self.df = df\n",
        "        self.target = target # HLM or MLM\n",
        "        self.is_test = is_test # train,valid / test\n",
        "        self.is_sigmoid = is_sigmoid\n",
        "        self.is_bce = is_bce\n",
        "\n",
        "        # self.feature_select = transform\n",
        "        if not self.is_test:\n",
        "            # self.fp = self.feature_select.fit_transform(np.stack(df['FPs']))\n",
        "            self.fp = np.stack(df['FPs'])\n",
        "        else: # valid or test\n",
        "            self.fp = np.stack(df['FPs'])\n",
        "            # self.fp = self.feature_select.transform(np.stack(df['FPs']))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fp = self.fp[index]\n",
        "        if not self.is_test: # test가 아닌 경우(label 존재)\n",
        "            if self.is_sigmoid and self.is_bce:\n",
        "              label = (self.df[self.target][index] / 100 + 0.5) / 2\n",
        "            else:\n",
        "              label = self.df[self.target][index]\n",
        "            return torch.tensor(fp).float().to(device), torch.tensor(label).float().unsqueeze(dim=-1).to(device) # feature, label\n",
        "\n",
        "        else: # test인 경우\n",
        "            return torch.tensor(fp).float().to(device) # feature\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "class gcnDataset(Dataset):\n",
        "    def __init__(self, df, max_len=120):\n",
        "        self.smiles = df[\"smiles\"]\n",
        "        self.exp = df[\"exp\"].values\n",
        "\n",
        "        list_X = list()\n",
        "        list_A = list()\n",
        "        for i, smiles in enumerate(self.smiles):\n",
        "            X, A = mol2graph(smiles, max_len)\n",
        "            list_X.append(X)\n",
        "            list_A.append(A)\n",
        "\n",
        "        self.X = np.array(list_X, dtype=np.uint8)\n",
        "        self.A = np.array(list_A, dtype=np.uint8)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.A[index], self.exp[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnPbcxQlFn1W"
      },
      "outputs": [],
      "source": [
        "transform = VarianceThreshold(threshold=0)\n",
        "# transform = False\n",
        "\n",
        "train_MLM = CustomDataset(df=train_data, target='MLM', transform=transform, is_test=False, is_sigmoid = CFG['SIGMOID'], is_bce = CFG['BCE'])\n",
        "train_HLM = CustomDataset(df=train_data, target='HLM', transform=transform, is_test=False, is_sigmoid = CFG['SIGMOID'], is_bce = CFG['BCE'])\n",
        "\n",
        "input_size = train_MLM.fp.shape[1]\n",
        "input_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWGOIUA2qQ8l"
      },
      "outputs": [],
      "source": [
        "CFG['INPUT_SIZE'] = input_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvOEkb-QFvmT"
      },
      "outputs": [],
      "source": [
        "# train,valid split\n",
        "train_MLM_dataset, valid_MLM        = train_test_split(train_MLM, test_size=0.2, random_state=seed)\n",
        "valid_MLM_dataset, test_MLM_dataset = train_test_split(valid_MLM, test_size=0.5, random_state=seed)\n",
        "train_HLM_dataset, valid_HLM        = train_test_split(train_HLM, test_size=0.2, random_state=seed)\n",
        "valid_HLM_dataset, test_HLM_dataset = train_test_split(valid_HLM, test_size=0.5, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBC1-xfuFzGB"
      },
      "outputs": [],
      "source": [
        "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
        "                              batch_size=CFG['BATCH_SIZE'],\n",
        "                              shuffle=True)\n",
        "\n",
        "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
        "                              batch_size=CFG['BATCH_SIZE'],\n",
        "                              shuffle=False)\n",
        "\n",
        "test_MLM_loader  = DataLoader(dataset=test_MLM_dataset,\n",
        "                              batch_size=CFG['BATCH_SIZE'],\n",
        "                              shuffle=False)\n",
        "\n",
        "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
        "                              batch_size=CFG['BATCH_SIZE'],\n",
        "                              shuffle=True)\n",
        "\n",
        "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
        "                              batch_size=CFG['BATCH_SIZE'],\n",
        "                              shuffle=False)\n",
        "\n",
        "test_HLM_loader  = DataLoader(dataset=test_HLM_dataset,\n",
        "                              batch_size=CFG['BATCH_SIZE'],\n",
        "                              shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD2zRHmcF0h0"
      },
      "outputs": [],
      "source": [
        "# @title Model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_rate, out_size, is_sigmoid, is_bce):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.is_sigmoid = is_sigmoid\n",
        "        self.is_bce = is_bce\n",
        "\n",
        "        # fc 레이어 3개와 출력 레이어\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size, device = device)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size, device = device)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size, device = device)\n",
        "        self.fc_out = nn.Linear(hidden_size, out_size, device = device)\n",
        "\n",
        "        # 정규화\n",
        "        self.ln1 = nn.LayerNorm(hidden_size, device = device)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size, device = device)\n",
        "        self.ln3 = nn.LayerNorm(hidden_size, device = device)\n",
        "\n",
        "        # 활성화 함수\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.ln1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.ln2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc3(out)\n",
        "        out = self.ln3(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        if self.is_sigmoid and self.is_bce:\n",
        "          out = self.sigmoid(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TW79HdqhLCJ"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_filter, out_filter, stride, use_bn, dp_rate, block_type):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.use_bn = use_bn\n",
        "        self.block_type = block_type\n",
        "        self.conv1 = nn.Conv2d(in_filter, out_filter, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_filter, out_filter, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(out_filter)\n",
        "        self.bn2 = nn.BatchNorm2d(out_filter)\n",
        "        self.dropout = nn.Dropout2d(dp_rate)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_filter != out_filter:\n",
        "            self.shortcut.add_module(\n",
        "                'conv', nn.Conv2d(in_filter, out_filter,\n",
        "                                  kernel_size=1, stride=stride,\n",
        "                                  padding=0, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, _x):\n",
        "        if self.block_type == 'a': #original residual block\n",
        "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
        "            x = self.bn2(self.conv2(x)) if self.use_bn else self.conv2(x)\n",
        "            x = x + self.shortcut(_x)\n",
        "            return self.dropout(self.relu(x))\n",
        "\n",
        "        elif self.block_type == 'b': # BN after addition\n",
        "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
        "            x = self.conv2(x) + self.shortcut(_x)\n",
        "            return self.dropout(self.relu(self.bn2(x)) if self.use_bn else self.relu(x))\n",
        "\n",
        "        elif self.block_type == 'c': # ReLU before addition\n",
        "            x = self.relu(self.bn1(self.conv1(_x))) if self.use_bn else self.relu(self.conv1(_x))\n",
        "            x = self.relu(self.bn2(self.conv2(x))) if self.use_bn else self.relu(self.conv2(x))\n",
        "            return self.dropout(x + self.shortcut(_x))\n",
        "\n",
        "        elif self.block_type == 'd': # ReLU-only pre-activation\n",
        "            x = self.bn1(self.conv1(self.relu(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
        "            x = self.bn2(self.conv2(self.relu(x))) if self.use_bn else self.conv2(self.relu(x))\n",
        "            return self.dropout(x + self.shortcut(_x))\n",
        "\n",
        "        elif self.block_type == 'e': # full pre-activation\n",
        "            x = self.conv1(self.relu(self.bn1(_x))) if self.use_bn else self.conv1(self.relu(_x))\n",
        "            x = self.conv2(self.relu(self.bn2(x))) if self.use_bn else self.conv2(self.relu(x))\n",
        "            return self.dropout(x + self.shortcut(_x))\n",
        "\n",
        "\n",
        "class CNNNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNNet, self).__init__()\n",
        "\n",
        "        # Create Atom Element embedding layer\n",
        "        self.embedding = self.create_emb_layer(args.vocab_size, args.emb_train)\n",
        "\n",
        "        # Create Residual Convolution layer\n",
        "        list_res_blocks = list()\n",
        "        n_channel = 1\n",
        "        for i in range(args.n_stage):\n",
        "            if i==0:\n",
        "                list_res_blocks.append(ResBlock(n_channel, n_channel*args.start_channel, args.stride, args.use_bn, args.dp_rate, args.block_type))\n",
        "                n_channel *= args.start_channel\n",
        "            else:\n",
        "                list_res_blocks.append(ResBlock(n_channel, n_channel*2, args.stride, args.use_bn, args.dp_rate, args.block_type))\n",
        "                n_channel *= 2\n",
        "            for j in range(args.n_layer-1):\n",
        "                list_res_blocks.append(ResBlock(n_channel, n_channel, 1, args.use_bn, args.dp_rate, args.block_type))\n",
        "        self.res_blocks = nn.Sequential(*list_res_blocks)\n",
        "\n",
        "        # Create MLP layers\n",
        "        fc_shape = self._estimate_fc_shape((1, args.max_len, ))\n",
        "        self.fc1 = nn.Linear(fc_shape[-1], 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3 = nn.Linear(50, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def create_emb_layer(self, vocab_size, emb_train):\n",
        "        emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
        "        weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
        "        for i in range(vocab_size):\n",
        "            weight_matrix[i][i] = 1\n",
        "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
        "\n",
        "        if not emb_train:\n",
        "            emb_layer.weight.requires_grad = False\n",
        "        return emb_layer\n",
        "\n",
        "    def _estimate_fc_shape(self, input_shape):\n",
        "        dummy_input = torch.zeros(input_shape).long()\n",
        "        dummy_output = self._conv_forward(dummy_input)\n",
        "        fc_shape = dummy_output.view(dummy_output.shape[0], -1).shape\n",
        "        return fc_shape\n",
        "\n",
        "    def _conv_forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = embeds.view(embeds.shape[0], 1, embeds.shape[1], embeds.shape[2])\n",
        "        x = self.res_blocks(embeds)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv_forward(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.squeeze(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsciw5OhhaVh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BN1d(nn.Module):\n",
        "    def __init__(self, out_dim, use_bn):\n",
        "        super(BN1d, self).__init__()\n",
        "        self.use_bn = use_bn\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.use_bn:\n",
        "            return  x\n",
        "        origin_shape = x.shape\n",
        "        x = x.view(-1, origin_shape[-1])\n",
        "        x = self.bn(x)\n",
        "        x = x.view(origin_shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GConv(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, use_bn):\n",
        "        super(GConv, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.bn = BN1d(output_dim, use_bn)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        x = self.fc(X)\n",
        "        x = torch.matmul(A, x)\n",
        "        x = self.relu(self.bn(x))\n",
        "        return x, A\n",
        "\n",
        "\n",
        "class Readout(nn.Module):\n",
        "    def __init__(self, out_dim, molvec_dim):\n",
        "        super(Readout, self).__init__()\n",
        "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
        "        nn.init.xavier_normal_(self.readout_fc.weight.data)\n",
        "\n",
        "    def forward(self, output_H):\n",
        "        molvec = self.readout_fc(output_H)\n",
        "        molvec = torch.mean(molvec, dim=1)\n",
        "        return molvec\n",
        "\n",
        "\n",
        "class GCNNet(nn.Module):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(GCNNet, self).__init__()\n",
        "\n",
        "        # Create Atom Element embedding layer\n",
        "        self.embedding = self.create_emb_layer([args.vocab_size, args.degree_size,\n",
        "                                                args.numH_size, args.valence_size,\n",
        "                                                args.isarom_size],  args.emb_train)\n",
        "\n",
        "        self.gcn_layers = nn.ModuleList()\n",
        "        for i in range(args.n_layer):\n",
        "            self.gcn_layers.append(GConv(args.in_dim if i==0 else args.out_dim, args.out_dim, args.use_bn))\n",
        "\n",
        "        self.readout = Readout(args.out_dim, args.molvec_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(args.molvec_dim, args.molvec_dim//2)\n",
        "        self.fc2 = nn.Linear(args.molvec_dim//2, args.molvec_dim//2)\n",
        "        self.fc3 = nn.Linear(args.molvec_dim//2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def create_emb_layer(self, list_vocab_size, emb_train=False):\n",
        "        list_emb_layer = nn.ModuleList()\n",
        "        for i, vocab_size in enumerate(list_vocab_size):\n",
        "            vocab_size += 1\n",
        "            emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
        "            weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
        "            for i in range(vocab_size):\n",
        "                weight_matrix[i][i] = 1\n",
        "            emb_layer.load_state_dict({'weight': weight_matrix})\n",
        "            emb_layer.weight.requires_grad = emb_train\n",
        "            list_emb_layer.append(emb_layer)\n",
        "        return list_emb_layer\n",
        "\n",
        "    def _embed(self, x):\n",
        "        list_embed = list()\n",
        "        for i in range(5):\n",
        "            list_embed.append(self.embedding[i](x[:, :, i]))\n",
        "        x = torch.cat(list_embed, 2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, A):\n",
        "        A = A.float()\n",
        "        x = self._embed(x)\n",
        "\n",
        "        for i, module in enumerate(self.gcn_layers):\n",
        "            x, A = module(x, A)\n",
        "        x = self.readout(x)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.squeeze(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxyObx3SGUcB"
      },
      "outputs": [],
      "source": [
        "# model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'],CFG['SIGMOID'],CFG['BCE']).to(device)\n",
        "# model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'],CFG['SIGMOID'],CFG['BCE']).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVxXXv5PGXv2"
      },
      "outputs": [],
      "source": [
        "# if CFG['SIGMOID']:\n",
        "#   criterion = nn.BCELoss()\n",
        "#   print('sigmoid')\n",
        "# else:\n",
        "#   criterion = nn.MSELoss()\n",
        "#   print('mse')\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
        "# optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVmTMnhrGZEk"
      },
      "outputs": [],
      "source": [
        "# @title Training\n",
        "\n",
        "# def train(train_loader, valid_loader, model, criterion, optimizer, epochs):\n",
        "#     model.train()\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         running_loss = 0\n",
        "#         for inputs, targets in train_loader:\n",
        "#             optimizer.zero_grad()\n",
        "#             output = model(inputs)\n",
        "#             loss = criterion(output, targets)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#         if epoch % 100 == 0:\n",
        "#             valid_loss = 0\n",
        "#             with torch.no_grad():\n",
        "#                 for inputs, targets in valid_loader:\n",
        "#                     output = model(inputs)\n",
        "#                     loss = criterion(output, targets)\n",
        "#                     valid_loss += loss.item()\n",
        "\n",
        "#             print(f'Epoch: {epoch}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Valid Loss: {valid_loss/len(valid_HLM_loader)}')\n",
        "\n",
        "#             model.train()\n",
        "\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMrmKcNZ08rv"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, args, **kwargs):\n",
        "\n",
        "    epoch_train_loss = 0\n",
        "    list_train_loss = list()\n",
        "    cnt_iter = 0\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        X, y = batch[0].long(), batch[1].float()\n",
        "        X, y = X.to(args.device), y.to(args.device)\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_y = model(X)\n",
        "        train_loss = criterion(pred_y, y)\n",
        "        epoch_train_loss += train_loss.item()\n",
        "        # list_train_loss.append({'epoch':batch_idx/len(dataloader)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cnt_iter += 1\n",
        "    return model, epoch_train_loss/cnt_iter\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, args):\n",
        "\n",
        "    epoch_val_loss = 0\n",
        "    cnt_iter = 0\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        X, y = batch[0].long(), batch[1].float()\n",
        "        X, y = X.to(args.device), y.to(args.device)\n",
        "\n",
        "        model.eval()\n",
        "        pred_y = model(X)\n",
        "        val_loss = criterion(pred_y, y)\n",
        "        epoch_val_loss += val_loss.item()\n",
        "        cnt_iter += 1\n",
        "\n",
        "    return epoch_val_loss/cnt_iter\n",
        "\n",
        "def test(model, dataloader, args, **kwargs):\n",
        "\n",
        "    list_y, list_pred_y = list(), list()\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        X, y = batch[0].long(), batch[1].float()\n",
        "        X, y = X.to(args.device), y.to(args.device)\n",
        "\n",
        "        model.eval()\n",
        "        pred_y = model(X)\n",
        "        list_y += y.cpu().detach().numpy().tolist()\n",
        "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
        "\n",
        "    mae = mean_absolute_error(list_y, list_pred_y)\n",
        "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
        "    return mae, std, list_y, list_pred_y\n",
        "\n",
        "\n",
        "def experiment(partition, args):\n",
        "    ts = time.time()\n",
        "    # args.input_shape = (args.max_len, args.vocab_size)\n",
        "\n",
        "    if args.model == 'CNN':\n",
        "      print('CNN')\n",
        "      model = CNNNet(args)\n",
        "    else:\n",
        "      model = Net(args)\n",
        "\n",
        "    model.to(args.device)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Initialize Optimizer\n",
        "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if args.optim == 'ADAM':\n",
        "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
        "    elif args.optim == 'RMSProp':\n",
        "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
        "    elif args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
        "    else:\n",
        "        assert False, \"Undefined Optimizer Type\"\n",
        "\n",
        "    # Train, Validate, Evaluate\n",
        "    list_train_loss = list()\n",
        "    list_val_loss = list()\n",
        "    list_mae = list()\n",
        "    list_std = list()\n",
        "\n",
        "    args.best_mae = 10000\n",
        "    for epoch in range(args.epoch):\n",
        "        model, train_loss = train(model, partition['train'], optimizer, criterion, args, **{'epoch':epoch})\n",
        "        val_loss = validate(model, partition['val'], criterion, args)\n",
        "        mae, std, true_y, pred_y = test(model, partition['test'], args, **{'epoch':epoch})\n",
        "\n",
        "        list_train_loss.append({'epoch':epoch, 'train_loss':train_loss})\n",
        "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
        "        list_mae.append({'epoch':epoch, 'mae':mae})\n",
        "        list_std.append({'epoch':epoch, 'std':std})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print('Epoch: {:2}/{:2}, Train Loss: {:2.3f}, Valid Loss: {:2.3f}'.format(epoch, args.epoch, train_loss, val_loss))\n",
        "\n",
        "        if args.best_mae > mae or epoch==0:\n",
        "            args.best_epoch = epoch\n",
        "            args.best_mae = mae\n",
        "            args.best_std = std\n",
        "            args.best_true_y = true_y\n",
        "            args.best_pred_y = pred_y\n",
        "\n",
        "\n",
        "    # End of experiments\n",
        "    te = time.time()\n",
        "    args.elapsed = te-ts\n",
        "    args.train_losses = list_train_loss\n",
        "    args.val_losses = list_val_loss\n",
        "    args.maes = list_mae\n",
        "    args.stds = list_std\n",
        "\n",
        "    return model, args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k21Ml17OIE4_"
      },
      "outputs": [],
      "source": [
        "CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aMCEKrHLrf4"
      },
      "outputs": [],
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sCvV0ens3P5k",
        "outputId": "18002916-3b91-461d-a11a-3527e263477e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "CNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0/100, Train Loss: 3621564.403, Valid Loss: 2676.864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100, Train Loss: 1382.875, Valid Loss: 1371.530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([238, 1])) that is different to the input size (torch.Size([238])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-dea077574bea>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# writer.write(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-9faa7be4ae9f>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(partition, args)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-9faa7be4ae9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# list_train_loss.append({'epoch':batch_idx/len(dataloader)+kwargs['epoch'], 'train_loss':train_loss.item()})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Experiments\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from utils import *\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "args.vocab_size = vocab_size + 2\n",
        "args.max_len = input_size\n",
        "\n",
        "args.n_layer = 1\n",
        "args.n_stage = 3\n",
        "\n",
        "args.lr = CFG['LEARNING_RATE']\n",
        "args.l2_coef = 0.0001\n",
        "args.optim = 'ADAM'\n",
        "args.epoch = 100\n",
        "args.test_batch_size= CFG['BATCH_SIZE']\n",
        "args.emb_train = False\n",
        "args.start_channel = 256\n",
        "args.stride = 2\n",
        "args.use_bn = True\n",
        "args.dp_rate = 0.3\n",
        "args.block_type = 'a'\n",
        "args.shuffle = True\n",
        "args.device = device\n",
        "args.model = 'CNN'\n",
        "print (device)\n",
        "\n",
        "args.batch_size = CFG['BATCH_SIZE']\n",
        "args.exp_name = 'exp1_lr_stage'\n",
        "\n",
        "\n",
        "# writer = Writer(prior_keyword=['n_layer', 'n_stage','block_type', 'use_bn', 'lr', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
        "#writer.clear()\n",
        "\n",
        "# Define Hyperparameter Search Space\n",
        "list_lr = [0.025, 0.005, 0.001]\n",
        "list_n_stage = [1,2,3,4,5]\n",
        "\n",
        "\n",
        "# train_dataloader = DataLoader(cnnDataset(datasets[0], vocab, args.max_len), batch_size=args.batch_size, shuffle=True)\n",
        "# val_dataloader = DataLoader(cnnDataset(datasets[1], vocab, args.max_len), batch_size=args.batch_size, shuffle=False)\n",
        "# test_dataloader = DataLoader(cnnDataset(datasets[2], vocab, args.max_len), batch_size=args.batch_size, shuffle=False)\n",
        "partition = {'train': train_MLM_loader, 'val': valid_MLM_loader, 'test' : test_MLM_loader}\n",
        "check_list = {}\n",
        "cnt_exp = 0\n",
        "for lr in list_lr:\n",
        "    for n_stage in list_n_stage:\n",
        "        args.lr = lr\n",
        "        args.n_stage = n_stage\n",
        "\n",
        "        model, result = experiment(partition, args)\n",
        "        # writer.write(result)\n",
        "\n",
        "        cnt_exp += 1\n",
        "        state_n = f'{n_stage}_{lr}'\n",
        "        check_list[stage_n] = result.best_mae\n",
        "        print('[Exp {:2}] got mae: {:2.3f}, std: {:2.3f} at epoch {:2}'.format(cnt_exp, result.best_mae, result.best_std, result.best_epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-VHuf0iGf9u"
      },
      "outputs": [],
      "source": [
        "print(\"Training Start: MLM\")\n",
        "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n",
        "\n",
        "print(\"Training Start: HLM\")\n",
        "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbJtdFy9Q21P"
      },
      "outputs": [],
      "source": [
        "# @title  Inference\n",
        "\n",
        "test_MLM = CustomDataset(df=test, target=None, transform=transform, is_test=True)\n",
        "test_HLM = CustomDataset(df=test, target=None, transform=transform, is_test=True)\n",
        "\n",
        "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
        "                             batch_size=CFG['BATCH_SIZE'],\n",
        "                             shuffle=False)\n",
        "\n",
        "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
        "                             batch_size=CFG['BATCH_SIZE'],\n",
        "                             shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FSUAXhMQ-B1"
      },
      "outputs": [],
      "source": [
        "def inference(test_loader, model, is_sigmoid):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in test_loader:\n",
        "            output = model(inputs)\n",
        "            if is_sigmoid:\n",
        "              output = (output * 2 - 0.5 ) * 100\n",
        "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG8LMsC_RB7y"
      },
      "outputs": [],
      "source": [
        "predictions_MLM = inference(test_MLM_loader, model_MLM, is_sigmoid = CFG['SIGMOID'])\n",
        "predictions_HLM = inference(test_HLM_loader, model_HLM, is_sigmoid = CFG['SIGMOID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRx-G2d1RGuf"
      },
      "outputs": [],
      "source": [
        "# @title Submission\n",
        "\n",
        "submission = pd.read_csv('data/sample_submission.csv')\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6FqlTy9RMs9"
      },
      "outputs": [],
      "source": [
        "submission['MLM'] = predictions_MLM\n",
        "submission['HLM'] = predictions_HLM\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGoJb3uTRPBL"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('result/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT6Gwlb7A18a"
      },
      "outputs": [],
      "source": [
        "mol = Chem.MolFromSmiles('CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW5Nu6UmCF2l"
      },
      "outputs": [],
      "source": [
        "Draw.MolsToGridImage([mol])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_0301qOCOhV"
      },
      "outputs": [],
      "source": [
        "rxn_positions = [Chem.MolFromSmarts(x.split('>>')[0]) for x in glory_rules.SMIRKS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vejBWjmc1je7"
      },
      "outputs": [],
      "source": [
        "rxn_positions_mols = [Chem.MolFromSmarts(x) for x in rxn_positions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m72PYUh91-Wr"
      },
      "outputs": [],
      "source": [
        "vec = [len(mol.GetSubstructMatches(x)) for x in rxn_positions_mols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVAPUtxP54Ef"
      },
      "outputs": [],
      "source": [
        "vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSnXDQAq6GBP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNPnM6QDeV+/FbrkA8uCaoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}